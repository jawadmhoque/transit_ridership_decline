{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "fetch_path = r'D:\\UoK\\OneDrive - University of Kentucky\\github\\Transit_ridership\\transit_ridership_decline\\Factors and Ridership Data\\Dependent Datasets'\n",
    "save_path = r'D:\\UoK\\OneDrive - University of Kentucky\\github\\Transit_ridership\\transit_ridership_decline\\Factors and Ridership Data\\Script Outputs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def read_PBS():\n",
    "    os.chdir(fetch_path)\n",
    "    df = pd.read_excel('CBSA_PBS.xlsx')\n",
    "\n",
    "    df['PBS_Start'] = np.where(df['PBS_Start'].isnull(), 0, df['PBS_Start'])\n",
    "    df['PBS_End'] = np.where(df['PBS_End'].isnull(), 0, df['PBS_End'])\n",
    "\n",
    "    df2 = pd.DataFrame(columns=['CBSAFP', 'Year', 'PBS_Start', 'PBS_End', 'PBS_Flag', 'Link'])\n",
    "    PBS_Scores = []\n",
    "    for index, row in df.iterrows():\n",
    "        x = 2002\n",
    "        result = 0\n",
    "        while x < 2019:\n",
    "            if row['PBS_Start'] != 0:\n",
    "                result = np.where(int(row['PBS_Start']) > x, 0, 1)\n",
    "            if row['PBS_End'] != 0:\n",
    "                result = np.where(row['PBS_End'] > x, 1, 0)\n",
    "            result_dict = {\n",
    "                'CBSAFP': str(row['CBSAFP']), 'Year': str(x), 'PBS_Start': str(row['PBS_Start']),\n",
    "                'PBS_End': str(row['PBS_End']), 'PBS_Flag': int(result), 'Link': str(row['Link'])\n",
    "            }\n",
    "            PBS_Scores.append(result_dict)\n",
    "            x += 1\n",
    "    # set the path to the dataset folder\n",
    "    os.chdir(fetch_path)\n",
    "    # Write the full results to csv using the pandas library.\n",
    "    pd.DataFrame(PBS_Scores).to_csv(\"PBS_Scores.csv\", encoding='utf8')\n",
    "    pd.DataFrame(PBS_Scores).to_excel(\"CBSA_PBS_Scores.xlsx\")\n",
    "    print(\"PBS file created sucessfully\")\n",
    "\n",
    "# get data for the dockless and scooters\n",
    "def get_cbsa_dockless_scooters():\n",
    "    os.chdir(fetch_path)\n",
    "    df = pd.read_csv('CBSA_Dockless&Scooter.csv')\n",
    "    # Group by and sum the docked, dockless and scooters based on CBSA code values and  years\n",
    "    df.groupby(['CBSAFP', 'Year']).sum().to_csv('summed_CBSA_copy.csv')\n",
    "    # _df = df.groupby(['CBSAFP', 'Year']).sum()\n",
    "#     _df.to_csv('summed_copy.csv')\n",
    "    _df=pd.read_csv('summed_CBSA_copy.csv')\n",
    "    # now return\n",
    "    return _df\n",
    "# \n",
    "# get data for the public bike sharing schemes (PBS) -- Note - this data is different from the dockless and scooters.\n",
    "def get_cbsa_pbs():\n",
    "    # read the Dockless and Scooter file and save it as excel\n",
    "    os.chdir(fetch_path)\n",
    "    df = pd.read_excel('CBSA_PBS_Scores.xlsx')\n",
    "    # now return\n",
    "    return df\n",
    "\n",
    "# get the walkscores for each city part of the analysis. If there are new cities please run the walkscore_script.py\n",
    "# before running this function\n",
    "def get_cbsa_walkscore():\n",
    "    # read the Dockless and Scooter file and save it as excel\n",
    "    os.chdir(fetch_path)\n",
    "    df = pd.read_excel('CBSA_Walkscore.xlsx')\n",
    "    # now return\n",
    "    print(\"read walkscore\")\n",
    "    return df\n",
    "\n",
    "# merge datasets Stage 1 --> Stage 2 --> Stage 3\n",
    "def merge_dataset_main_dockless_scooters(_df1, _df2):\n",
    "    df = pd.merge(_df1, _df2[['CBSAFP', 'Year', 'dockCt','docklessCt','scooterCt']], how='left', left_on=['CBSA', 'Year'],\n",
    "                  right_on=['CBSAFP', 'Year'], indicator=True)\n",
    "    print(\"Dockless sucess\")\n",
    "    return df\n",
    "\n",
    "# def merge_dataset_main_pbs(_df1, _df2):\n",
    "#     _df1 = _df1.drop(columns=['_merge'])\n",
    "#     df = pd.merge(_df1, _df2[['CBSAFP', 'As_of', 'PBS_Start', 'PBS_End']], how='left', left_on=['CBSA'],\n",
    "#                   right_on=['CBSAFP'], indicator=False)\n",
    "#     print(\"PBS sucess\")\n",
    "#     return df                                                                                 \n",
    "\n",
    "def merge_dataset_main_pbs(_df1, _df2):\n",
    "    _df1 = _df1.drop(columns=['_merge'])\n",
    "    df = pd.merge(_df1, _df2[['CBSAFP', 'Year', 'PBS_Start', 'PBS_End', 'PBS_Flag', 'Link']], how='left', left_on=['CBSA', 'Year'],\n",
    "                  right_on=['CBSAFP', 'Year'], indicator=False)\n",
    "    print(\"PBS sucess\")\n",
    "    return df\n",
    "\n",
    "def merge_dataset_main_walkscore(_df1, _df2):\n",
    "    _df1 = _df1.drop(columns=['CBSAFP_x'])\n",
    "    df = pd.merge(_df1, _df2[['CBSAFP', 'Year', 'walkscore', 'transit', 'bike']], how='left', left_on=['CBSA', 'Year'],\n",
    "                  right_on=['CBSAFP', 'Year'], indicator=False)\n",
    "    print(\"Walkscore sucess\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def prepare_charts(_mergeddf):\n",
    "    df = _mergeddf\n",
    "\n",
    "    # rename the columns\n",
    "    df.rename(columns={'CBSA': 'CBSACode'}, inplace=True)\n",
    "\n",
    "    # sort values based on city_id and then year\n",
    "    df = df.sort_values(['CBSACode', 'Year'], ascending=[True, False])\n",
    "\n",
    "    # get unique values of years and city_ids\n",
    "    yrs = df['Year'].unique()\n",
    "    yrs.sort()\n",
    "    id2s = df['CBSACode'].unique()\n",
    "    id2s.sort()\n",
    "\n",
    "    # TNC_year=\"\"\n",
    "    x = 1\n",
    "    for _id2s in id2s:\n",
    "        # df_fltr_id2 = df[df['CBSACode'] == _id2s]\n",
    "        df_fltr_id2 = df[df.CBSACode == _id2s]\n",
    "        print(_id2s)\n",
    "\n",
    "        # get unique modes in the city\n",
    "        modes = df_fltr_id2['Mode'].unique()\n",
    "        modes.sort()\n",
    "\n",
    "        col_index = df_fltr_id2.columns.get_loc(\"MNAME\")\n",
    "        city_name = str(df_fltr_id2.iloc[0, col_index])\n",
    "        print('Cityname:' + str(city_name))\n",
    "\n",
    "        df_fltr_id2['Year'] = pd.to_datetime(df_fltr_id2['Year'].astype(str), format='%Y')\n",
    "\n",
    "        df_fltr_id2 = df_fltr_id2.set_index(pd.DatetimeIndex(df_fltr_id2['Year']).year)\n",
    "\n",
    "        # get number of sub-plots defined - 4*3 means 4 rows having 3 graphs (each sized 18x9) in each row = 12 graphs\n",
    "        fig, ax = plt.subplots(nrows=4, ncols=3, figsize=(18, 9))\n",
    "\n",
    "        # Year vs Total Population --> Graph (0,1)\n",
    "        df_fltr_id2.groupby('CBSACode').plot(x='Year', y='Tot_Pop', ax=ax[0][0], legend=True)\n",
    "        ax[0][0].set(xlabel=\"Years\", ylabel=\"Total population\")\n",
    "        ax[0][0].legend(loc='best')\n",
    "\n",
    "#         # Year vs HH with no veh\n",
    "        df_fltr_id2.groupby('CBSACode').plot(x='Year', y='PCT_HH_NO_VEH', ax=ax[1][0], legend=True)\n",
    "        ax[1][0].set(xlabel=\"Years\", ylabel=\"HH with no vehicles (in percentage)\")\n",
    "#         ax[0][1].legend(loc='best')\n",
    "\n",
    "        # Year vs Gas price\n",
    "        df_fltr_id2.groupby('CBSACode').plot(x='Year', y='GAS_PRICE_2018', ax=ax[3][0], legend=True)\n",
    "        ax[3][0].set(xlabel=\"Years\", ylabel=\"Gas Price ($) \")\n",
    "        ax[3][0].legend(loc='best')\n",
    "\n",
    "        # Year vs Income levels\n",
    "        df_fltr_id2.groupby('CBSACode').plot(x='Year', y=r'INC_U35', ax=ax[2][0], legend=True, marker='', color='skyblue', linewidth=2)\n",
    "        df_fltr_id2.groupby('CBSACode').plot(x='Year', y=r'INC_35_100', ax=ax[2][0], legend=True, marker='', color='olive', linewidth=2)\n",
    "        df_fltr_id2.groupby('CBSACode').plot(x='Year', y=r'INC_100P', ax=ax[2][0], legend=True, marker='', color='yellow', linewidth=2)\n",
    "        ax[2][0].set(xlabel=\"Years\", ylabel='INC_Levels($)')\n",
    "        ax[2][0].legend(loc='best')\n",
    "\n",
    "        # Year vs VRM_ADJ, UPT_ADJ, FARE_PER_UPT\n",
    "        # iterate and differentiate graph items mode wise -- Rail or Bus?\n",
    "        for _mode in modes:\n",
    "            df_fltr_modes = df_fltr_id2[df_fltr_id2['Mode'] == _mode]\n",
    "            \n",
    "            # Year vs Avg. speed\n",
    "            df_fltr_modes.groupby('Mode').plot(x='Year', y='AVG_SPEED', label=str(_mode), ax=ax[2][1], legend=True)\n",
    "#             df_fltr_id2.groupby('CBSACode').plot(x='Year', y='AVG_SPEED', ax=ax[1][0], legend=True)\n",
    "            ax[2][1].set(xlabel=\"Years\", ylabel='Avg Speed (mph)')\n",
    "            ax[2][1].legend(loc='best')\n",
    "\n",
    "            df_fltr_modes.groupby('Mode').plot(x='Year', y='VRM_ADJ', label=str(_mode), ax=ax[3][1], legend=True)\n",
    "            ax[3][1].set(xlabel=\"Years\", ylabel='VRMs')\n",
    "            ax[3][1].legend(loc='best')\n",
    "\n",
    "            df_fltr_modes.groupby('Mode').plot(x='Year', y='UPT_ADJ', label=('UPT - ' + str(_mode)), ax=ax[0][1],legend=True)\n",
    "            ax[0][1].set(xlabel=\"Years\", ylabel='Rdrship')\n",
    "            ax[0][1].legend(loc='best')\n",
    "\n",
    "            df_fltr_modes.groupby('Mode').plot(x='Year', y='FARE_per_UPT_2018', label=('Fare/Rdr - ' + str(_mode)), ax=ax[1][1], legend=True)\n",
    "            ax[1][1].set(xlabel=\"Years\", ylabel='Fare per rdrship')\n",
    "            ax[1][1].legend(loc='best')\n",
    "\n",
    "        # # Year vs Dock count - Dockless and Scooters\n",
    "        df_fltr_id2.groupby('CBSACode').plot(x='Year', y='dockCt', ax=ax[2][2], legend=True, marker='', color='skyblue',linewidth=2)\n",
    "        df_fltr_id2.groupby('CBSACode').plot(x='Year', y='docklessCt', ax=ax[2][2], legend=True, marker='',color='olive', linewidth=2)\n",
    "        df_fltr_id2.groupby('CBSACode').plot(x='Year', y='scooterCt', ax=ax[2][2], legend=True, marker='',color='yellow', linewidth=2)\n",
    "        ax[2][2].set(xlabel=\"Years\", ylabel='PBS_Dockless_Scooters_details')\n",
    "        ax[2][2].legend(loc='best')\n",
    "        \n",
    "        # Year vs TNC Presence\n",
    "        df_fltr_id2.groupby('CBSACode').plot(x='Year', y='TNC_FLAG', ax=ax[0][2], legend=True)\n",
    "        ax[0][2].set(xlabel=\"Years\", ylabel='TNC Presence')\n",
    "        ax[0][2].legend(loc='best')\n",
    "\n",
    "        # Year vs Walkscore\n",
    "        df_fltr_id2.groupby('CBSACode').plot(x='Year', y='walkscore', ax=ax[3][2], legend=True, marker='',color='skyblue',linewidth=2)\n",
    "        df_fltr_id2.groupby('CBSACode').plot(x='Year', y='transit', ax=ax[3][2], legend=True, marker='', color='olive',linewidth=2)\n",
    "        df_fltr_id2.groupby('CBSACode').plot(x='Year', y='bike', ax=ax[3][2], legend=True, marker='', color='yellow',linewidth=2)\n",
    "        left,right = ax[3][2].get_xlim()\n",
    "        ax[3][2].set_xlim(left+1, right+1)       \n",
    "        ax[3][2].set(xlabel=\"Years\", ylabel='Walk Scores')\n",
    "        ax[3][2].legend(loc='best')\n",
    "        \n",
    "#         df_fltr_id2['PBS_Start'] = pd.to_datetime(df_fltr_id2['PBS_Start'],format='%Y')\n",
    "#         df_fltr_id2['PBS_End'] = pd.to_datetime(df_fltr_id2['PBS_End'], format='%Y')\n",
    "        # Year vs PBS_Start and End_Date\n",
    "#         PBS_prs_yr = df_fltr_id2['PBS_Start'].unique().tolist() \n",
    "#         col_index_PBS = df_fltr_id2.columns.get_loc(\"PBS_Start\") \n",
    "#         df_fltr_id2['PBS_Start'] = pd.to_datetime(df_fltr_id2['PBS_Start'].astype(str), format='%Y')\n",
    "#         if TNC_prs_yr:\n",
    "#             ax[3][2].axvline(PBS_prs_yr[0],color='green', linewidth=1)\n",
    "        df_fltr_id2.groupby('CBSACode').plot(x='Year', y='PBS_Flag', ax=ax[1][2], legend=True)\n",
    "#         df_fltr_id2.groupby('CBSACode').plot(x='Year', y='PBS_Start', ax=ax[1][2], legend=False, marker='^',color='skyblue',linewidth=2)\n",
    "#         df_fltr_id2.groupby('CBSACode').plot(x='Year', y='PBS_End', ax=ax[1][2], legend=False, marker='v',color='yellow',linewidth=2)        \n",
    "        left,right = ax[1][2].get_xlim()\n",
    "        ax[1][2].set_xlim(left+2, right+2)\n",
    "        ax[1][2].set(xlabel=\"Years\")\n",
    "        ax[1][2].legend(loc='best')\n",
    "        \n",
    "        fig.suptitle(city_name,fontsize=14)\n",
    "        fig.tight_layout()\n",
    "        _figno = x\n",
    "\n",
    "        # setting path to the dataset folder\n",
    "        os.chdir(save_path)\n",
    "        #     if os.path.exists((\"Charts\\\\\"+ \"Fig \" + str(_figno) + \" - \" + city_name + \".png\")):\n",
    "        #         os.remove((\"Charts\\\\\"+ \"Fig \" + str(_figno) + \" - \" + city_name + \".png\"))\n",
    "\n",
    "        # chartname = (\"Charts\\\\\" + \"Fig \" + str(_figno) + \" - \" + city_name + \".png\")\n",
    "        fig.savefig((\"Charts\\\\\" + \"Fig \" + str(_figno) + \" - \" + city_name + \".png\"))\n",
    "        plt.suptitle(city_name,fontsize=14)\n",
    "#         plt.title(city_name)\n",
    "        plt.close(fig)\n",
    "        x += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PBS file created sucessfully\n",
      "Dockless sucess\n",
      "PBS sucess\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'merged_df_3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-e5f6cc7407c4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-4-e5f6cc7407c4>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;31m# cast columsn into INTEGER\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m     \u001b[0mmerged_df_3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'merged_df_2.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m     \u001b[0mmerged_df_3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'TNC_FLAG'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'int32'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'merged_df_3' is not defined"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # set the path to the dataset folder\n",
    "    fetch_path = r'D:\\UoK\\OneDrive - University of Kentucky\\github\\Transit_ridership\\transit_ridership_decline\\Factors and Ridership Data\\Dependent Datasets'\n",
    "    save_path = r'D:\\UoK\\OneDrive - University of Kentucky\\github\\Transit_ridership\\transit_ridership_decline\\Factors and Ridership Data\\Script Outputs'\n",
    "\n",
    "    os.chdir(fetch_path)\n",
    "    # read excel file - main dataset\n",
    "    df = pd.read_csv('estimation_file.csv')\n",
    "    read_PBS()\n",
    "      \n",
    "    \n",
    "    # Stage 1 - Merge the main dataset with CBSA_Dockless&Scooters data. Please note - here the order of the\n",
    "    # parameters is important\n",
    "    df_dockless_scooters = get_cbsa_dockless_scooters()\n",
    "    merged_df_1 = merge_dataset_main_dockless_scooters(df, df_dockless_scooters)\n",
    "    merged_df_1.to_csv('merged_df_1.csv')\n",
    "\n",
    "    # Stage 2 - Merge the modified dataset with CBSA_PBS data. Please note - here the order of the\n",
    "    # parameters is important\n",
    "    df_pbs = get_cbsa_pbs()\n",
    "    merged_df_2 = merge_dataset_main_pbs(merged_df_1, df_pbs)\n",
    "    merged_df_2.to_csv('merged_df_2.csv')\n",
    "\n",
    "    # Stage 3 - Merge the modified dataset with Walkscore data. Please note - here the order of the\n",
    "    # parameters is important\n",
    "#     df_walkscore = get_cbsa_walkscore()    \n",
    "    merged_df_3 = merge_dataset_main_walkscore(merged_df_2, df_walkscore)\n",
    "    print(\"after merged_df_3\")\n",
    "    merged_df_3 = merged_df_3.drop(columns=['CBSAFP_y', 'CBSAFP'])\n",
    "    \n",
    "    # cast columsn into INTEGER\n",
    "    \n",
    "    merged_df_3.astype({'TNC_FLAG': 'int32'}).dtypes   \n",
    "\n",
    "    merged_df_3['MNAME'] = np.where(merged_df_3['MNAME']=='Louisville/Jefferson County, KY-IN Metro Area', 'Louisville, Jefferson County, KY-IN Metro Area', merged_df_3['MNAME'])\n",
    "    # Edit dataset\n",
    "    # set default values of the already existing columns\n",
    "#     merged_df_3['UPT'] = np.where(merged_df_3['UPT'].isnull(), '0', merged_df_3['UPT'])\n",
    "#     merged_df_3['VRM'] = np.where(merged_df_3['VRM'].isnull(), '0', merged_df_3['VRM'])\n",
    "\n",
    "    merged_df_3['UPT'] = np.where(merged_df_3['UPT_ADJ'].isnull(), '0', merged_df_3['UPT_ADJ'])\n",
    "    merged_df_3['VRM'] = np.where(merged_df_3['VRM_ADJ'].isnull(), '0', merged_df_3['VRM_ADJ'])\n",
    "\n",
    "    merged_df_3['Pop_Below150_Poverty'] = np.where(merged_df_3['Pop_Below150_Poverty'].isnull(), '0', merged_df_3['Pop_Below150_Poverty'])\n",
    "    merged_df_3['Pop_Above150_Poverty'] = np.where(merged_df_3['Pop_Above150_Poverty'].isnull(), '0', merged_df_3['Pop_Above150_Poverty'])\n",
    "\n",
    "    merged_df_3['walkscore'] = np.where(merged_df_3['walkscore'].isnull(), 0, merged_df_3['walkscore'])\n",
    "    \n",
    "    merged_df_3['bike'] = np.where(merged_df_3['bike'].isnull(), 0, merged_df_3['bike'])\n",
    "    \n",
    "    merged_df_3['transit'] = np.where(merged_df_3['transit'].isnull(), 0, merged_df_3['transit'])\n",
    "    \n",
    "    merged_df_3['dockCt'] = np.where(merged_df_3['dockCt'].isnull(), 0, merged_df_3['dockCt'])\n",
    "    \n",
    "    merged_df_3['docklessCt'] = np.where(merged_df_3['docklessCt'].isnull(), 0, merged_df_3['docklessCt'])\n",
    "    \n",
    "    merged_df_3['scooterCt'] = np.where(merged_df_3['scooterCt'].isnull(), 0, merged_df_3['scooterCt'])\n",
    "    \n",
    "# #     merged_df_3['dockNm'] = np.where(merged_df_3['dockNm'] == '-',None, merged_df_3['dockNm'])\n",
    "    \n",
    "# #     merged_df_3['docklessNm'] = np.where(merged_df_3['docklessNm'] == '-', None, merged_df_3['docklessNm'])\n",
    "    \n",
    "# #     merged_df_3['scooterNm'] = np.where(merged_df_3['scooterNm'] == '', None, merged_df_3['scooterNm'])\n",
    "    \n",
    "# #     merged_df_3['Type'] = np.where(merged_df_3['Type'] == '', None, merged_df_3['Type'])\n",
    "\n",
    "    merged_df_3['PBS_Start'] = np.where(merged_df_3['PBS_Start']== 0, None, merged_df_3['PBS_Start'])\n",
    "    # merged_df_3['PBS_Start'] = np.where(merged_df_3['PBS_Start'] == '-', '0', merged_df_3['PBS_Start'])\n",
    "\n",
    "    merged_df_3['PBS_End'] = np.where(merged_df_3['PBS_End']== 0, None, merged_df_3['PBS_End'])\n",
    "    # merged_df_3['PBS_End'] = np.where(merged_df_3['PBS_End'] == '-', '0', merged_df_3['PBS_End'])\n",
    "    # merged_df_3.to_csv('merged_df_3.csv')\n",
    "    \n",
    "    merged_df_3.astype({'walkscore': 'int32'}).dtypes\n",
    "    merged_df_3.astype({'bike': 'int32'}).dtypes\n",
    "    merged_df_3.astype({'transit': 'int32'}).dtypes\n",
    "    \n",
    "    merged_df_3.astype({'dockCt': 'int32'}).dtypes\n",
    "    merged_df_3.astype({'docklessCt': 'int32'}).dtypes\n",
    "    merged_df_3.astype({'scooterCt': 'int32'}).dtypes\n",
    "    \n",
    "    os.chdir(fetch_path)\n",
    "    merged_df_3.to_csv('merged_df_3.csv')\n",
    "    \n",
    "    prepare_charts(merged_df_3)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
